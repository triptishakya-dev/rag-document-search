# ğŸ“„ DocSearch+ (RAG Document Search)

DocSearch+ is a powerful **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDF documents and intelligently chat with them. By leveraging semantic vector search and Large Language Models (LLMs), the system provides accurate, context-aware answers grounded solely in your uploaded content.

The architecture is designed for performance and reliability, offloading heavy PDF processing to background workers to keep the user interface lightning-fast.

---

## ğŸš€ Key Features

-   **ğŸ“„ Intelligent Ingestion**: Securely upload PDFs with automated text extraction and high-quality chunking.
-   **âš¡ Async Processing**: Redis-powered task queue (BullMQ) handles PDF parsing and embedding without blocking the API.
-   **ğŸ” Semantic Search**: Vector embeddings stored in **Qdrant DB** enable precise retrieval of relevant context.
-   **ğŸ’¬ AI-Powered Chat**: Grounded responses generated by **Google Gemini 2.5 Flash**, including source citations.
-   **ğŸ“¦ Cloud Storage**: Reliable file persistence using **Amazon S3**.
-   **ğŸ—„ï¸ Structured Persistence**: Metadata and chunk tracking managed via **PostgreSQL** & **Prisma ORM**.

---

## ğŸ› ï¸ Modern Tech Stack

Our stack is categorized for clarity and architectural separation:

### ğŸ¨ Frontend
- **Next.js 14**: Modern React framework for performance and App Router convenience.
- **Tailwind CSS**: Utility-first styling for a premium, responsive interface.
- **Lucide Icons**: Clean and consistent iconography.

### âš™ï¸ Backend
- **Node.js (Express)**: Scalable, asynchronous API server.
- **BullMQ**: Robust Redis-backed queue for reliable background processing.
- **Prisma**: Type-safe ORM for relational data management.

### ğŸ¤– AI & Vector
- **Google Gemini 2.5**: State-of-the-art LLM for embeddings and answer generation.
- **Qdrant DB**: High-performance vector database optimized for similarity search.
- **LangChain**: Orchestrating document loading and retrieval pipelines.

### â˜ï¸ Infrastructure
- **Amazon S3**: Secure object storage for PDF assets.
- **PostgreSQL**: Relational storage for document metadata.
- **Redis**: Fast, in-memory data store for the message queue.

---

## ğŸ—ï¸ Architecture: How It Works

### 1. Document Ingestion Pipeline
1.  **Upload**: User uploads a PDF via the UI; handled by **Multer** on the server.
2.  **Enqueue**: The upload task is pushed to a **Redis Queue**.
3.  **Process**: A background **Worker** picks up the task:
    -   Uploads the raw file to **AWS S3**.
    -   Saves document records in **PostgreSQL**.
    -   Splits text into chunks and generates embeddings via **Gemini**.
4.  **Index**: Embeddings and content are upserted into **Qdrant DB**.

### 2. Chat & Retrieval Pipeline
1.  **Ask**: User submits a question via the chat interface.
2.  **Embed**: The question is converted into a vector using the **Gemini Embedding** model.
3.  **Retrieve**: A similarity search is performed in **Qdrant** to find relevant text chunks.
4.  **Generate**: The question and retrieved context are sent to **Gemini 2.5 Flash** to generate a grounded response.

---

## ğŸ—ï¸ Folder Structure

```
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ controllers     # API business logic (chat, upload)
â”‚   â”œâ”€â”€ lib             # Core services (S3, Embedding, Queue)
â”‚   â”œâ”€â”€ routes          # Express route definitions
â”‚   â”œâ”€â”€ index.js        # API Entry point
â”‚   â”œâ”€â”€ worker.js       # Background processing worker
â”‚   â””â”€â”€ prisma          # DB Schema & Migrations
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ app             # App Router pages (Chat, Upload, How-it-works)
â”‚   â”œâ”€â”€ components      # Shadcn/UI & custom components
â”‚   â””â”€â”€ lib             # Client-side utilities
```

---

## âš™ï¸ Setup & Installation

### Prerequisites
- **Node.js** (v18+)
- **Docker** (Recommended for infra)
- **Gemini API Key** (from Google AI Studio)

### 1. Infrastructure (via Docker)
```bash
docker run -p 6379:6379 -d redis
docker run -p 6333:6333 -d qdrant/qdrant
```

### 2. Backend Setup
```bash
cd backend
npm install
```
Configure your `.env`:
```env
DATABASE_URL="postgresql://user:password@localhost:5432/docsearch"
GEMINI_API_KEY="your_key_here"
BUCKET_NAME="your_s3_bucket"
REDIS_HOST="localhost"
# ... + S3 Credentials
```

### 3. Running the App
- **Start API**: `npm run dev` (in `backend`)
- **Start Worker**: `node worker.js` (in `backend`)
- **Start UI**: `npm run dev` (in `frontend`)

---

## ğŸ”Œ API Summary

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| `POST` | `/api/upload-documents` | Upload and queue a PDF for processing. |
| `POST` | `/api/chat` | Chat with your documents using semantic search. |
| `GET` | `/api/health` | Check system status. |
